import matplotlib.pyplot as plt
# %matplotib inline


import math;
import random;
row = 0
col = 0
weights = [[0 for i in range(9)] for j in range(9)]


weights[1][3] = 0.8 
weights[1][4] = 0.5
weights[1][5] = -0.1
weights[1][6] = 0.7
    
weights[2][3] = -0.3
weights[2][4] = 0.1
weights[2][5] = 0.3
weights[2][6] = 0.9

weights[3][7] = -0.8
weights[3][8] = 0.4
    
weights[4][7] = -0.9
weights[4][8] = -0.4
    
weights[5][7] = -0.6
weights[5][8] = -0.5

weights[6][7] = -0.7
weights[6][8] = -0.2

biasFor3 = -0.9
biasFor4 = -0.8
biasFor5 = 0.8
biasFor6 = -0.9
biasFor7 = 0.7
biasFor8 = 0.4

# rowForFirstDataset is used to interate the entire file 
# and cover all the data points
def trainingDatatset(fileName):
    global row, col;
    with open (fileName) as trainingDataset:
        Davids_Database_During_2020 = [];
        for line in trainingDataset:
            dataSet = [item.strip() for item in line.split(' ')]
            Davids_Database_During_2020.append(dataSet);
        # row = len(Davids_Database_During_2020);
        # col = len(Davids_Database_During_2020[0]);
    return Davids_Database_During_2020;
#################Changing Data########################################
#For trainingDataset 1
trainingDatatset1 = trainingDatatset("DataMining/Neural_Network/trainingDataset1.txt")
rowForFirstDataset = 0;
inputX = float(trainingDatatset1[rowForFirstDataset][2])
inputY = float(trainingDatatset1[rowForFirstDataset][3])

#For testing dataset 1
# trainingDatatset1 = trainingDatatset("DataMining/Neural_Network/testingDataset1.txt")
# rowForFirstDataset = 0;
# inputX = float(trainingDatatset1[rowForFirstDataset][2])
# inputY = float(trainingDatatset1[rowForFirstDataset][3])

# For trainingDataset 2
trainingDatatset2 = trainingDatatset("DataMining/Neural_Network/trainingDataset2.txt")
# rowForFirstDataset = 0;
# inputX = float(trainingDatatset1[rowForFirstDataset][0])
# inputY = float(trainingDatatset1[rowForFirstDataset][1])

#For testing dataset 2
# trainingDatatset1 = trainingDatatset("DataMining/Neural_Network/testingDatatset2.txt")
# rowForFirstDataset = 0;
# inputX = float(trainingDatatset1[rowForFirstDataset][0])
# inputY = float(trainingDatatset1[rowForFirstDataset][1])

#################Changing Data########################################

# Start of Forward propagation
########################################
    # Test case professors values
    #  DataEntries = [1,0,1]
    #     weights = [0.2, -0.3, 0.4, 0.1, -0.5, 0.2, -0.3, -0.2]
    #     Bias = [-0.4, 0.2, 0.1]
        
    #     #               (0.2 * 1)                     +(-0.4*0)                         + (-0.5*1)                    + (-0.4)
    #     newInputFor4 = (weights[0] * DataEntries[0]) + (weights[2] * DataEntries[1]) + (weights[4] * DataEntries[2]) + Bias[0];
    #     outputFor4 = round(1/(1 + math.exp(-(newInputFor4))),3);
        
        
        # print(newInputFor4);
        # print(outputFor4)
    #put into forWardpropagation for results  
########################################

# In the diagram we only have each node in the hidden layer recieves data from 2 nodes
# for example node 3 is getting data from 1 and 2

def outputForNodeCalc(weight1, weight2, inputX, inputY, bias):
   netInput =  (weight1 * inputX) + (weight2 * inputY) + bias;
#    print("(%0.3f * %0.3f) + (%0.3f * %0.3f) + %0.3f = %0.3f" %(weight1, inputX, weight2, inputY, bias, netInput))
   output = round(1/(1 + math.exp(-(netInput))),3);
#    print("1/(1 + e^-(%0.3f)) = %f" %(round(netInput, 3), output ))
   return output;
# outputFor_7 = (weights[3][7] * outputFor_3) + (weights[4][7] * outputFor_4) + (weights[5][7] * outputFor_5) + (weights[6][7] * outputFor_6) + biasFor7
def outputForOuterNode(weight1, weight2, weight3, weight4, input1, input2, input3, input4, bias):
    netInput = (weight1 * input1) + (weight2 * input2) + (weight3 * input3) + (weight4 * input4) + bias
    # print("(%0.3f * %0.3f) + (%0.3f * %0.3f) + (%0.3f * %0.3f) + (%0.3f * %0.3f) + %0.3f = %0.3f" %(weight1, input1, weight2, input2, weight3, input3, weight4, input4, bias, netInput))
    output = round(1/(1 + math.exp(-(netInput))),3);
    # print("1/(1 + e^-(%0.3f)) = %f" %(round(netInput, 3), output ))
    return output
# creating a table to display for data after forward propagation
def tableConstruction(hiddenlayer, outputs):
    table = [[0 for i in range(2)] for j in range(7)]

    table[0][0] = "Unit j"
    table[1][0] = hiddenlayer[0]
    table[2][0] = hiddenlayer[1]
    table[3][0] = hiddenlayer[2]
    table[4][0] = hiddenlayer[3]
    table[5][0] = hiddenlayer[4]
    table[6][0] = hiddenlayer[5]
    
    table[0][1] = "Outputs"
    table[1][1] = outputs[0]
    table[2][1] = outputs[1]
    table[3][1] = outputs[2]
    table[4][1] = outputs[3]
    table[5][1] = outputs[4]
    table[6][1] = outputs[5]

    return table;
# End of Forward propagation

def forwardPropagation():
    global weights, inputX, inputY, biasFor3, biasFor4, biasFor5, biasFor6, biasFor7, biasFor8;
    trainingDataset1 = trainingDatatset("DataMining/Neural_Network/trainingDataset2.txt");
    # The while loops makes sure each weight is unique
    # The loops will do continious check to make sure all the values are unique 
    # This same will have to the biases
    # setting the weights for 3,4,5,6,7,8

        # calculating the output for each Hidden layer and and and outter nodes
        # 3,4,5,6,7,8

        # 16 is the number of weight the entire network has
    # weights = [0 for i in range(16)]
    
    # random weight generator
    # for i in range(len(weights)):
    #     weight = round(random.uniform(-0.9,0.9),1);
    #     while(weight in weights):
    #         weight = round(random.uniform(-0.9,0.9),1);
    #     weights[i] = weight


    # print(weights);
    # The weight are ordred in the forlling order. The is a Static sample data set that could be used to compute the alkgorithm 
    # [0.8, 0.5, -0.1, 0.7, -0.3, 0.1, 0.3, 0.9, -0.8, 0.4, -0.9, -0.4, -0.6, -0.5, -0.7, -0.2]
    # weight[1][3] = weight1_3

    




    
    outputFor_3 = outputForNodeCalc(weights[1][3], weights[2][3], inputX, inputY,  biasFor3)
    outputFor_4 = outputForNodeCalc(weights[1][4], weights[2][4], inputX, inputY,  biasFor4)
    outputFor_5 = outputForNodeCalc(weights[1][5], weights[2][5], inputX, inputY,  biasFor5)
    outputFor_6 = outputForNodeCalc(weights[1][6], weights[2][6], inputX, inputY,  biasFor6)

    outputFor_7 = outputForOuterNode(weights[3][7], weights[4][7], weights[5][7], weights[6][7], outputFor_3, outputFor_4, outputFor_5, outputFor_6, biasFor7)
    outputFor_8 = outputForOuterNode(weights[3][8], weights[4][8], weights[5][8], weights[6][8], outputFor_3, outputFor_4, outputFor_5, outputFor_6, biasFor8)

    hiddenLayers = [3,4,5,6,7,8]
    outputs = [outputFor_3,outputFor_4,outputFor_5,outputFor_6,outputFor_7,outputFor_8]
    forwardPropagationTable = tableConstruction(hiddenLayers, outputs)
    return forwardPropagationTable;
    


# Start of backpropagation

# This fuction will take in the following
# 1. The ouputs for the for the outer nodes
# 2. The output for the inner nodes that are pointing to the outer node
# 3. weather the node being inserted is a outernode or not will be represented by 
# boolean value -> outerNode
# 4. It will also recieve the weight  
# This will be split into multiple functions 
def errorCalculationForOuter(output1):
    errorForOuterNodes = (output1) * (1-output1) * (1-output1) 
    # print("Error Calculation for outer nodes")
    # print("%0.3f * (1-%0.3f) * (1 - %0.3f) = %0.3f"%(output1,output1,output1, errorForOuterNodes))
    return errorForOuterNodes;
# error or parent represent the nodes this node is pointing to
def errorCalculationForinnerNodes(errorOfParent1, errorOfParent2, output, weight1, weight2):
    # print("Error calculation for Inner nodes")
    errorCalculationForinnerNodes = round(output * (1 - output) * ((errorOfParent1) * (weight1) + (errorOfParent2) * (weight2)), 3)
    # round(errorCalculationForinnerNodes, 4)
    # print("%0.3f * (1-%0.3f) * ((%0.3f * %0.3f) + (%0.3f * %0.3f)) = %0.3f" %(output, output, errorOfParent1, weight1, errorOfParent2, weight2, errorCalculationForinnerNodes))
    return errorCalculationForinnerNodes;

# This function is update the weights 
# The weigts will be updated dependent of 3 values
    # 1. Current weight
    # 2. Learning Rate set to 0.8
    # 3. Output of their parent or the node its pointing to
    # 4. Output of the node itself 
def update_weights(currentWeight, learningRate,outputOfParent, outputOfNode):
    updated_weight = round(currentWeight + learningRate * outputOfParent * outputOfNode,3);
    # print("%0.3f + %0.3f * %0.3f * %0.3f" %(currentWeight, learningRate, outputOfParent, outputOfNode));
    return updated_weight

def bias_errorRate(currectBiasErrorRate, learningRate, errorRateOFNode):
    updatedBias = currectBiasErrorRate + learningRate * errorRateOFNode;
    return updatedBias;

def back_propagation():
    global inputX, inputY, weights, biasFor3, biasFor4, biasFor5, biasFor6, biasFor7, biasFor8;

    learningRate = 0.8
    # Retriving the values of forward propagation 
    outputTable = forwardPropagation();
    errorOF_7 = errorCalculationForOuter(outputTable[5][1])
    errorOF_8 = errorCalculationForOuter(outputTable[6][1])
    
    errorOF_3 = errorCalculationForinnerNodes(errorOF_7, errorOF_8, outputTable[1][1], weights[3][7], weights[3][8])
    errorOF_4 = errorCalculationForinnerNodes(errorOF_7, errorOF_8, outputTable[2][1], weights[4][7], weights[4][8])
    errorOF_5 = errorCalculationForinnerNodes(errorOF_7, errorOF_8, outputTable[3][1], weights[5][7], weights[5][8])
    errorOF_6 = errorCalculationForinnerNodes(errorOF_7, errorOF_8, outputTable[4][1], weights[6][7], weights[6][8])

    # print("%0.3f + %0.3f * %0.3f * %0.3f = " %(weights[3][7], learningRate, outputTable[5][1], outputTable[1][1]))
    # print("before")
    # print(weights[3][7], weights[4][7], weights[5][7], weights[6][7],
    #       weights[3][8], weights[4][8], weights[5][8], weights[6][8]
    # )

    weights[3][7] = update_weights(weights[3][7], learningRate, errorOF_7, outputTable[1][1]);
    weights[4][7] = update_weights(weights[4][7], learningRate, errorOF_7, outputTable[2][1]);
    weights[5][7] = update_weights(weights[5][7], learningRate, errorOF_7, outputTable[3][1]);
    weights[6][7] = update_weights(weights[6][7], learningRate, errorOF_7, outputTable[4][1]);

    weights[3][8] = update_weights(weights[3][8], learningRate, errorOF_8, outputTable[1][1]);
    weights[4][8] = update_weights(weights[3][8], learningRate, errorOF_8, outputTable[2][1]);
    weights[5][8] = update_weights(weights[3][8], learningRate, errorOF_8, outputTable[3][1]);
    weights[6][8] = update_weights(weights[3][8], learningRate, errorOF_8, outputTable[4][1]);

    # print("After")
    # print(weights[3][7], weights[4][7], weights[5][7], weights[6][7],
    #       weights[3][8], weights[4][8], weights[5][8], weights[6][8]
    # )
    weights[1][3] = update_weights(weights[1][3], learningRate, errorOF_3, inputX);
    weights[1][4] = update_weights(weights[1][4], learningRate, errorOF_4, inputX);
    weights[1][5] = update_weights(weights[1][5], learningRate, errorOF_5, inputX);
    weights[1][6] = update_weights(weights[1][6], learningRate, errorOF_6, inputX);

    weights[2][3] = update_weights(weights[2][3], learningRate, errorOF_3, inputY);
    weights[2][4] = update_weights(weights[2][4], learningRate, errorOF_4, inputY);
    weights[2][5] = update_weights(weights[2][5], learningRate, errorOF_5, inputY);
    weights[2][6] = update_weights(weights[2][6], learningRate, errorOF_6, inputY);
    
    # print(biasFor3, biasFor4, biasFor5, biasFor6, biasFor7, biasFor8)
    #updating the bias
    biasFor3 = bias_errorRate(biasFor3, learningRate, errorOF_3);
    biasFor4 = bias_errorRate(biasFor4, learningRate, errorOF_4);
    biasFor5 = bias_errorRate(biasFor5, learningRate, errorOF_5);
    biasFor6 = bias_errorRate(biasFor6, learningRate, errorOF_6);
    biasFor7 = bias_errorRate(biasFor7, learningRate, errorOF_7);
    biasFor8 = bias_errorRate(biasFor8, learningRate, errorOF_8);

    # print(biasFor3, biasFor4, biasFor5, biasFor6, biasFor7, biasFor8)

# end of backpropagation

def results():
    i = 0;
    global inputX, inputY, rowForFirstDataset, inputX, inputY
    output1 = [0 for i in range(len(trainingDatatset1))] 
    output2 = [0 for i in range(len(trainingDatatset1))] 

    while(rowForFirstDataset < len(trainingDatatset1)): 
         
        print("Initial Outputs after first propagation ")
        print("Dataset being used: (%0.3f, %0.3f), row = %d" %(inputX, inputY, rowForFirstDataset))
        Initial_Array = forwardPropagation();
        for row in Initial_Array:
            print(row)
        output1[rowForFirstDataset-1] = Initial_Array[5][1];
        output2[rowForFirstDataset-1] = Initial_Array[6][1];
        print("-------------------------------------------");
        print("Outputs after 1000th forward and backward propagation ")
        print("Dataset being used: (%0.3f, %0.3f)" %(inputX, inputY))
        while(i < 10):
            back_propagation();
            Initial_Array2 = forwardPropagation();
            i+=1
        
        for row in Initial_Array2:
            print(row)

        
        print("###############################################")
        print("###############################################")
        
        rowForFirstDataset += 1;
        if((rowForFirstDataset + 1) <= len(trainingDatatset1)):
            inputX = float(trainingDatatset1[rowForFirstDataset][0])
            inputY = float(trainingDatatset1[rowForFirstDataset][1])  
    print(output1)
    print("\n\n")
    print(output2)

def AccuracyCalCulation():
    #trainingDataset1
    trainingDataset1Outputs1 = [0.786, 0.787, 0.769, 0.781, 0.786, 0.774, 0.777, 0.782, 0.791, 0.785, 0.781, 0.783, 0.712, 0.783, 0.789, 0.783, 0.772, 0.779, 0.779, 0.793, 0.766, 0.782, 0.785, 0.788, 0.787, 0.791, 0.782, 0.782, 0.778, 0.679, 0.78, 0.78, 0.79, 0.791, 
        0.783, 0.787, 0.77, 0.771, 0.779, 0.777, 0.794, 0.791, 0.787, 0.784, 0.782, 0.776, 0.768, 0.776, 0.774, 0.778, 0.779, 
        0.78, 0.783, 0.774, 0.788, 0.778, 0.776, 0.784, 0.784, 0.777, 0.782, 0.783, 0.794, 0.781, 0.784, 0.782, 0.789, 0.782, 
        0.787, 0.786, 0.786, 0.776, 0.772, 0.663, 0.785, 0.786, 0.788, 0.782, 0.781, 0.786, 0.78, 0.776, 0.781, 0.779, 0.776, 
        0.777, 0.782, 0.782, 0.782, 0.774, 0.793, 0.781, 0.775, 0.792, 0.78, 0.788, 0.775, 0.788, 0.78, 0.784, 0.724, 0.776, 0.778, 0.79, 0.785, 0.775, 0.787, 0.78, 0.772, 0.784, 0.786, 0.784, 0.774, 0.786, 0.789, 0.772, 0.796, 0.789, 0.78, 0.782, 0.772, 0.784, 0.773, 0.772, 0.781, 0.775, 0.783, 0.782, 0.769, 0.776, 0.783, 0.781, 0.699, 0.79, 0.78, 0.779, 0.792, 0.788, 0.779, 0.783, 0.754, 0.762, 0.791, 0.778, 0.782, 
        0.683, 0.761, 0.78, 0.792, 0.787, 0.785, 0.777, 0.779, 0.787, 0.785, 0.778, 0.782, 0.77, 0.704, 0.786, 0.774, 0.787, 0.776, 0.775, 0.795, 0.782, 0.71, 0.785, 0.772, 0.766, 0.785, 0.793, 0.79, 0.777, 0.779, 0.78, 0.784, 0.791, 0.77, 0.784, 0.786, 0.689, 0.787, 0.784, 0.78, 0.789, 0.776, 0.783, 0.788, 0.717, 0.79, 0.691, 0.777, 0.777, 0.664, 0.783, 0.789, 0.786, 0.79, 0.772, 0.776, 0.788, 0.778, 0.785, 0.784, 0.78, 0.785, 0.782, 0.79, 0.769, 0.78, 0.782, 0.778, 0.767, 0.792, 0.783, 0.753, 0.795, 0.779, 0.756, 0.68, 0.781, 0.772, 0.786, 0.786, 0.785, 0.784, 0.784, 0.78, 0.786, 0.782, 0.789, 0.777, 0.775, 0.782, 0.789, 0.778, 0.778, 0.786, 0.787, 0.768, 0.792, 0.774, 0.778, 0.782, 0.78, 0.785, 0.767, 0.79, 0.774, 0.784, 0.773, 0.719, 0.773, 0.786, 0.787, 0.777, 0.78, 0.776, 0.671, 0.785, 0.792, 0.788, 0.786, 0.788, 0.793, 0.702, 0.79, 0.79, 0.784, 0.785, 0.782, 0.705, 0.777, 0.684, 0.781, 0.775, 0.699, 0.778, 0.761, 0.775, 0.779, 0.773, 0.784, 0.764, 0.769, 0.781, 0.678, 0.783, 0.78, 0.784, 0.693, 0.776, 0.697, 0.77, 0.68, 0.78, 0.785, 0.789, 0.719, 
        0.676, 0.666, 0.682, 0.686, 0.721, 0.69, 0.684, 0.7, 0.721, 0.674, 0.708, 0.687, 0.701, 0.697, 0.68, 0.664, 0.681, 0.677, 0.698, 0.701, 0.692, 0.733, 0.707, 0.711, 0.687, 0.723, 0.684, 0.692, 0.711, 0.7, 0.674, 0.682, 0.713, 0.708, 0.696, 0.69, 0.687, 0.715, 0.691, 0.674, 0.696, 0.716, 0.695, 0.697, 0.673, 0.714, 0.701, 0.691, 0.694, 0.719, 0.694, 0.686, 0.69, 0.671, 0.698, 0.671, 0.692, 0.704, 0.731, 0.711, 0.697, 0.691, 0.714, 0.696, 0.684, 0.679, 0.735, 0.714, 0.671, 0.677, 0.712, 0.695, 0.741, 0.704, 0.709, 0.695, 0.697, 
        0.681, 0.686, 0.663, 0.679, 0.704, 0.702, 0.695, 0.687, 0.697, 0.687, 0.658, 0.704, 0.685, 0.699, 0.719, 0.706, 0.713, 0.701, 0.691, 0.682, 0.695, 0.696, 0.673, 0.705, 0.701, 0.686, 0.707, 0.699, 0.7, 0.698, 0.694, 0.686, 0.685, 0.699, 
        0.686, 0.709, 0.734, 0.676, 0.721, 0.705, 0.71, 0.695, 0.728, 0.713, 0.697, 0.682, 0.714, 0.679, 0.69, 0.702, 0.709, 0.707, 0.702, 0.702, 0.682, 0.671, 0.724, 0.71, 0.678, 0.701, 0.716, 0.701, 0.686, 0.657, 0.718, 0.697, 0.714, 0.706, 0.701, 0.71, 0.68, 0.705, 0.7, 0.702, 0.683, 0.692, 0.673, 0.71, 0.691, 0.723, 0.703, 0.691, 0.697, 0.715, 0.691, 0.706, 0.663, 0.688, 0.691, 0.682, 0.729, 0.697, 0.681, 0.709, 0.693, 0.682, 0.695, 0.674, 0.721, 0.705, 0.663, 0.672, 0.671, 0.687, 0.701, 0.688, 0.677, 0.664, 0.708, 0.698, 0.705, 
        0.701, 0.677, 0.689, 0.695, 0.696, 0.718, 0.712, 0.686, 0.698, 0.713, 0.686, 0.221]
    
    trainingDataset1Outputs2 = [0.833, 0.829, 0.839, 0.841, 0.835, 0.845, 0.841, 0.837, 0.836, 0.833, 0.841, 0.836, 0.842, 0.835, 0.84, 0.828, 0.83, 
        0.838, 0.838, 0.838, 0.846, 0.842, 0.839, 0.839, 0.833, 0.835, 0.832, 0.821, 0.841, 0.865, 0.837, 0.836, 0.837, 0.837, 0.829, 0.836, 0.847, 0.846, 0.837, 0.84, 0.833, 0.834, 0.835, 0.843, 0.834, 0.837, 0.845, 0.838, 0.841, 0.822, 0.844, 0.839, 0.838, 0.836, 0.834, 0.834, 0.843, 0.841, 0.838, 0.84, 0.84, 0.839, 0.838, 0.841, 0.827, 0.843, 0.834, 0.828, 
        0.838, 0.825, 0.84, 0.842, 0.846, 0.875, 0.838, 0.838, 0.832, 0.819, 0.839, 0.833, 0.842, 0.838, 0.835, 0.845, 0.844, 
        0.833, 0.827, 0.833, 0.835, 0.842, 0.838, 0.838, 0.843, 0.833, 0.834, 0.833, 0.827, 0.837, 0.841, 0.83, 0.832, 0.831, 
        0.84, 0.839, 0.838, 0.844, 0.835, 0.828, 0.827, 0.829, 0.839, 0.834, 0.838, 0.838, 0.837, 0.844, 0.833, 0.834, 0.838, 
        0.842, 0.846, 0.829, 0.839, 0.843, 0.834, 0.844, 0.832, 0.841, 0.845, 0.846, 0.833, 0.837, 0.852, 0.836, 0.838, 0.834, 0.833, 0.84, 0.838, 0.837, 0.845, 0.844, 0.829, 0.84, 0.842, 0.863, 0.848, 0.828, 0.828, 0.839, 0.831, 0.841, 0.843, 
        0.839, 0.837, 0.837, 0.842, 0.838, 0.848, 0.839, 0.844, 0.836, 0.841, 0.841, 0.837, 0.841, 0.843, 0.84, 0.845, 0.85, 0.834, 0.829, 0.826, 0.836, 0.834, 0.838, 0.825, 0.832, 0.844, 0.836, 0.841, 0.859, 0.838, 0.842, 0.839, 0.833, 0.841, 
        0.834, 0.84, 0.838, 0.838, 0.857, 0.842, 0.836, 0.874, 0.838, 0.834, 0.84, 0.841, 0.843, 0.84, 0.83, 0.841, 0.841, 0.826, 0.841, 0.838, 0.836, 0.84, 0.835, 0.837, 0.825, 0.83, 0.847, 0.831, 0.836, 0.851, 0.835, 0.844, 0.849, 0.865, 0.837, 0.837, 0.831, 0.832, 0.834, 0.825, 0.835, 0.841, 0.841, 
        0.823, 0.834, 0.846, 0.838, 0.84, 0.832, 0.844, 0.836, 0.831, 0.837, 0.847, 0.835, 0.841, 0.843, 0.831, 0.836, 0.84, 0.829, 0.83, 0.846, 0.84, 0.847, 0.836, 0.833, 0.837, 0.824, 0.843, 0.827, 0.834, 0.87, 0.824, 0.832, 0.834, 0.84, 0.834, 0.835, 0.849, 0.837, 0.83, 0.839, 0.834, 0.839, 0.847, 0.844, 0.862, 0.824, 0.845, 0.852, 0.841, 0.851, 0.845, 0.841, 0.848, 0.836, 0.848, 0.83, 0.841, 0.866, 0.843, 0.84, 0.84, 0.856, 0.839, 0.853, 0.84, 0.865, 0.824, 0.832, 0.84, 0.853, 0.864, 0.878, 0.876, 0.873, 0.866, 0.875, 0.878, 0.865, 0.86, 0.87, 0.866, 0.873, 0.866, 0.871, 0.871, 0.874, 0.874, 0.878, 0.874, 0.862, 0.864, 0.861, 0.854, 0.865, 0.863, 0.865, 0.875, 0.855, 0.847, 0.863, 0.883, 0.88, 0.868, 0.865, 0.846, 0.869, 0.873, 0.862, 0.874, 0.873, 0.875, 0.866, 0.877, 0.868, 0.877, 0.848, 0.873, 0.865, 0.874, 0.852, 0.856, 0.879, 0.872, 0.879, 0.873, 0.879, 0.871, 0.87, 0.856, 0.865, 0.861, 0.876, 0.859, 0.87, 0.876, 0.872, 0.846, 0.868, 0.87, 0.878, 0.854, 0.867, 0.856, 0.866, 0.852, 0.861, 0.874, 0.876, 0.871, 0.882, 0.881, 0.861, 0.87, 0.876, 0.874, 0.871, 0.875, 0.881, 0.87, 0.88, 0.867, 0.857, 0.861, 0.872, 0.866, 0.879, 0.875, 0.856, 0.872, 0.881, 0.857, 0.858, 0.866, 0.866, 0.873, 0.854, 0.875, 0.859, 0.88, 0.868, 0.868, 0.875, 0.872, 0.838, 0.879, 0.863, 0.867, 0.869, 0.874, 0.849, 0.84, 0.873, 0.879, 0.864, 0.88, 0.873, 0.859, 0.849, 0.87, 0.869, 0.87, 0.874, 0.864, 0.861, 0.869, 0.868, 
        0.873, 0.861, 0.865, 0.875, 0.868, 0.847, 0.875, 0.865, 0.869, 0.87, 0.865, 0.879, 0.866, 0.872, 0.87, 0.872, 0.84, 0.88, 0.863, 0.876, 0.841, 0.868, 0.862, 0.866, 0.863, 0.876, 0.854, 0.875, 0.867, 0.87, 0.875, 0.859, 0.875, 0.876, 0.869, 0.871, 0.878, 0.877, 0.869, 0.84, 0.862, 0.87, 0.878, 0.872, 0.863, 0.859, 0.877, 0.866, 0.875, 0.87, 0.867, 0.869, 0.849, 0.87, 0.849, 0.866, 0.875, 0.865, 0.86, 0.869, 0.86, 0.867, 0.864, 0.427]
    
    #testingDataset1
    testingDataset1Outputs1 = [0.705, 0.783, 0.787, 0.785, 0.79, 0.77, 0.779, 0.723, 0.782, 0.788, 0.778, 0.78, 0.778, 0.783, 0.78, 0.776, 0.791, 0.773, 0.778, 0.787, 0.705, 0.777, 0.786, 0.794, 0.785, 0.795, 0.781, 0.694, 0.784, 0.787, 0.776, 0.763, 
        0.782, 0.778, 0.79, 0.786, 0.78, 0.769, 0.781, 0.779, 0.699, 0.787, 0.696, 0.789, 0.784, 0.783, 0.791, 0.786, 0.777, 0.783, 0.783, 0.779, 0.787, 0.687, 0.784, 0.78, 0.792, 0.781, 0.779, 0.776, 0.787, 0.784, 0.781, 0.788, 0.775, 0.716, 0.787, 0.768, 0.767, 0.789, 0.776, 0.778, 0.79, 0.774, 0.788, 0.764, 0.785, 0.785, 0.788, 0.773, 0.775, 0.787, 0.778, 0.787, 0.787, 0.782, 0.784, 0.672, 0.773, 0.787, 0.789, 0.779, 0.789, 0.786, 0.782, 0.783, 0.778, 0.778, 0.789, 0.69, 0.721, 0.685, 0.694, 0.701, 0.698, 0.715, 0.691, 0.707, 0.671, 0.71, 0.704, 0.687, 0.709, 0.716, 0.695, 0.691, 0.658, 0.711, 0.676, 0.713, 0.687, 0.703, 0.705, 0.684, 0.716, 0.69, 0.73, 0.712, 0.675, 0.715, 0.693, 0.695, 0.722, 0.738, 0.701, 0.729, 0.707, 0.701, 0.71, 0.7, 0.687, 0.715, 0.703, 0.715, 0.706, 0.667, 0.686, 0.704, 0.69, 0.709, 0.704, 0.684, 0.707, 0.674, 0.709, 0.686, 0.69, 0.679, 0.702, 0.721, 0.697, 0.699, 0.724, 0.675, 0.68, 0.71, 0.69, 0.709, 0.691, 0.686, 0.689, 0.693, 0.714, 0.7, 0.708, 0.691, 0.682, 0.711, 0.695, 0.193]
    testingDataset1Outputs2 = [0.845, 0.84, 0.832, 0.838, 0.836, 0.845, 0.841, 0.831, 0.836, 0.834, 0.829, 0.83, 0.829, 0.839, 0.835, 0.818, 0.833, 0.841, 0.836, 0.835, 0.846, 0.821, 0.839, 0.833, 0.825, 0.832, 0.839, 0.855, 0.833, 0.834, 0.834, 0.847, 0.834, 0.838, 0.829, 0.835, 0.842, 0.842, 0.841, 0.837, 0.851, 0.828, 0.853, 0.834, 0.83, 0.835, 0.835, 0.834, 0.841, 0.833, 0.831, 0.837, 0.837, 0.859, 0.84, 0.839, 0.832, 0.836, 0.838, 0.818, 0.831, 0.837, 0.827, 0.833, 0.842, 0.836, 0.831, 0.846, 0.841, 0.836, 0.831, 0.838, 0.837, 0.836, 0.836, 0.841, 0.835, 0.839, 0.838, 0.84, 0.824, 0.825, 0.837, 0.827, 0.836, 0.834, 0.838, 0.869, 0.842, 0.838, 0.832, 0.839, 0.835, 0.831, 0.84, 0.829, 0.836, 0.841, 0.832, 0.867, 0.865, 0.867, 0.872, 0.871, 0.869, 0.866, 0.871, 0.869, 0.879, 0.866, 0.872, 0.877, 0.863, 0.867, 0.87, 0.868, 0.874, 0.862, 0.878, 0.864, 0.872, 0.868, 0.856, 0.859, 0.854, 0.875, 0.862, 0.865, 0.877, 0.862, 0.864, 0.873, 0.846, 0.857, 0.87, 0.858, 0.866, 0.87, 0.864, 0.859, 
        0.851, 0.866, 0.871, 0.862, 0.86, 0.878, 0.873, 0.866, 0.855, 0.865, 0.872, 0.865, 0.857, 0.88, 0.862, 0.861, 0.862, 0.867, 0.865, 0.851, 0.862, 0.861, 0.857, 0.864, 0.873, 0.868, 0.865, 0.867, 0.872, 0.87, 0.876, 0.873, 0.855, 0.87, 0.863, 0.873, 0.866, 0.868, 0.871, 0.42]

    #trainingDataset2
    trainingDataset2Outputs1 = [0.729, 0.732, 0.726, 0.729, 0.731, 0.715, 0.655, 0.655, 0.692, 0.631, 0.614, 0.764, 0.728, 0.759, 0.63, 0.676, 0.679, 0.621, 0.758, 0.668, 0.616, 0.727, 0.623, 0.685, 0.72, 0.759, 0.641, 0.716, 0.681, 0.679, 0.708, 0.704, 0.765, 0.684, 0.724, 0.631, 0.654, 0.755, 0.721, 0.627, 0.666, 0.709, 0.665, 0.624, 0.709, 0.682, 0.625, 0.686, 0.719, 0.714, 0.718, 0.684, 0.727, 0.721, 0.684, 0.679, 0.723, 0.699, 
        0.763, 0.713, 0.731, 0.717, 0.76, 0.683, 0.729, 0.734, 0.692, 0.728, 0.764, 0.753, 0.758, 
        0.647, 0.718, 0.683, 0.76, 0.744, 0.728, 0.709, 0.747, 0.731, 0.732, 0.702, 0.729, 0.685, 
        0.723, 0.724, 0.734, 0.645, 0.729, 0.693, 0.683, 0.714, 0.719, 0.718, 0.758, 0.649, 0.72, 
        0.721, 0.68, 0.752, 0.725, 0.722, 0.723, 0.73, 0.705, 0.714, 0.695, 0.659, 0.709, 0.716, 0.725, 0.756, 0.612, 0.631, 0.728, 0.634, 0.714, 0.652, 0.749, 0.621, 0.722, 0.717, 0.645, 
        0.681, 0.689, 0.711, 0.721, 0.76, 0.708, 0.72, 0.647, 0.649, 0.745, 0.717, 0.631, 0.747, 0.751, 0.735, 0.637, 0.686, 0.706, 0.735, 0.655, 0.742, 0.685, 0.666, 0.737, 0.75, 0.723, 0.705, 0.731, 0.681, 0.731, 0.689, 0.7, 0.633, 0.762, 0.686, 0.69, 0.684, 0.752, 0.755, 0.678, 0.686, 0.721, 0.72, 0.653, 0.726, 0.704, 
        0.728, 0.724, 0.718, 0.745, 0.68, 0.687, 0.703, 0.729, 0.707, 0.725, 0.628, 0.661, 0.666, 
        0.743, 0.704, 0.723, 0.684, 0.665, 0.698, 0.677, 0.704, 0.733, 0.717, 0.731, 0.691, 0.717, 0.678, 0.714, 0.624, 0.727, 0.727, 0.757, 0.681, 0.671, 0.628, 0.764, 0.727, 0.631, 0.765, 0.74, 0.686, 0.682, 0.694, 0.743, 0.67, 0.684, 0.751, 0.685, 0.706, 0.72, 0.667, 0.705, 
        0.678, 0.646, 0.701, 0.732, 0.618, 0.727, 0.727, 0.751, 0.728, 0.721, 0.729, 0.684, 0.683, 0.67, 0.739, 0.72, 0.637, 0.644, 0.711, 0.756, 0.75, 0.621, 0.748, 0.765, 0.687, 0.718, 0.768, 0.731, 0.636, 0.683, 0.743, 0.634, 0.678, 0.616, 0.695, 0.616, 0.726, 0.731, 0.722, 
        0.746, 0.632, 0.715, 0.654, 0.693, 0.724, 0.689, 0.673, 0.652, 0.636, 0.763, 0.696, 0.721, 0.764, 0.721, 0.727, 0.681, 0.681, 0.709, 0.724, 0.69, 0.698, 0.723, 0.615, 0.735, 0.619, 0.769, 0.635, 0.746, 0.616, 0.718, 0.693, 0.704, 0.619, 0.636, 0.755, 0.739, 0.712, 0.692, 0.688, 0.764, 0.626, 0.648, 0.746, 0.667, 0.736, 0.683, 0.72, 0.765, 0.745, 0.698, 0.765, 0.714, 0.701, 0.756, 0.73, 0.696, 0.723, 0.681, 0.757, 0.695, 0.727, 0.741, 0.686, 0.759, 0.726, 0.714, 0.722, 0.715, 0.765, 0.749, 0.695, 0.692, 0.65, 0.741, 0.723, 0.642, 0.731, 0.7, 0.715, 0.698, 0.681, 0.765, 0.764, 0.712, 0.61, 0.688, 0.751, 0.619, 0.682, 0.73, 0.639, 0.765, 0.677, 0.681, 0.727, 0.734, 0.708, 0.727, 0.743, 0.702, 0.726, 0.653, 0.634, 
        0.756, 0.677, 0.765, 0.766, 0.686, 0.727, 0.713, 0.717, 0.692, 0.727, 0.69, 0.642, 0.679, 
        0.671, 0.672, 0.681, 0.682, 0.766, 0.731, 0.755, 0.632, 0.682, 0.765, 0.619, 0.763, 0.701, 0.733, 0.707, 0.737, 0.686, 0.684, 0.617, 0.678, 0.759, 0.705, 0.675, 0.622, 0.738, 0.742, 0.765, 0.686, 0.677, 0.694, 0.676, 0.707, 0.704, 0.767, 0.734, 0.723, 0.638, 0.727, 0.663, 0.751, 0.761, 0.717, 0.763, 0.719, 0.72, 0.687, 0.65, 0.676, 0.678, 0.628, 0.724, 0.723, 0.624, 0.726, 0.75, 0.715, 0.7, 0.623, 0.712, 0.68, 0.682, 0.624, 0.717, 0.622, 0.728, 0.726, 0.679, 0.718, 0.708, 0.693, 0.691, 0.707, 0.672, 0.69, 0.717, 0.761, 0.751, 0.725, 0.716, 0.649, 0.758, 0.76, 0.688, 0.731, 0.684, 0.72, 0.73, 0.632, 0.721, 0.654, 0.696, 0.732, 0.715, 0.645, 0.686, 0.731, 0.688, 0.715, 0.762, 0.705, 0.722, 0.739, 0.705, 0.692, 0.726, 0.629, 0.729, 0.723, 0.725, 0.761, 0.632, 0.68, 0.718, 0.757, 0.713, 0.715, 0.695, 0.628, 0.66, 0.699, 0.706, 0.7, 0.64, 0.757, 0.635, 0.625, 0.71, 0.767, 0.655, 0.765, 0.713, 0.684, 0.725, 0.73, 0.722, 0.644, 0.652, 0.697, 0.623, 0.717, 0.764, 0.743, 0.75, 0.677, 
        0.618, 0.757, 0.742, 0.759, 0.683, 0.724, 0.736, 0.683, 0.625, 0.678, 0.761, 0.713, 0.73, 
        0.693, 0.685, 0.726, 0.724, 0.72, 0.666, 0.624, 0.683, 0.686, 0.717, 0.727, 0.679, 0.7, 0.656, 0.734, 0.683, 0.68, 0.708, 0.712, 0.697, 0.707, 0.729, 0.723, 0.624, 0.691, 0.72, 0.709, 0.722, 0.623, 0.622, 0.754, 0.683, 0.767, 0.679, 0.675, 0.679, 0.729, 0.646, 0.738, 0.654, 0.731, 0.73, 0.613, 0.666, 0.627, 0.727, 0.71, 0.719, 0.725, 0.754, 0.613, 0.67, 0.663, 0.698, 0.755, 0.633, 0.724, 0.704, 0.729, 
        0.727, 0.749, 0.646, 0.726, 0.636, 0.69, 0.725, 0.616, 0.727, 0.619, 0.763, 0.635, 0.707, 
        0.724, 0.697, 0.696, 0.67, 0.744, 0.65, 0.763, 0.612, 0.705, 0.706, 0.693, 0.707, 0.72, 0.65, 0.729, 0.709, 0.613, 0.721, 0.702, 0.706, 0.727, 0.657, 0.724, 0.643, 0.693, 0.719, 0.751, 0.726, 0.634, 0.711, 0.671, 0.679, 0.678, 0.72, 0.694, 0.65, 0.682, 0.689, 0.624, 0.674, 0.737, 0.725, 0.703, 0.685, 0.611, 0.726, 0.719, 0.644, 0.684, 0.696, 0.656, 0.675, 0.63, 0.678, 0.62, 0.692, 0.76, 0.678, 0.76, 0.625, 0.707, 0.683, 0.69, 0.695, 0.697, 0.727, 0.722, 0.71, 0.614, 0.711, 0.695, 0.724, 0.678, 0.747, 0.758, 0.705, 0.683, 0.727, 0.691, 0.685, 0.675, 0.658, 0.681, 0.713, 0.701, 0.711, 0.713, 0.764, 0.752, 0.691, 0.713, 0.726, 0.622, 0.727, 0.724, 0.733, 0.69, 0.7, 0.67, 0.688, 0.623, 0.677, 0.75, 0.749, 0.703, 0.75, 0.682, 0.663, 0.734, 0.73, 0.724, 0.677, 
        0.655, 0.686, 0.722, 0.721, 0.698, 0.684, 0.682, 0.732, 0.611, 0.627, 0.681, 0.676, 0.696, 0.715, 0.639, 0.712, 0.728, 0.704, 0.701, 0.623, 0.708, 0.634, 0.725, 0.728, 0.688, 0.682, 0.725, 0.722, 0.765, 0.686, 0.621, 0.626, 0.761, 0.748, 0.724, 0.681, 0.715, 0.687, 0.717, 0.69, 0.76, 0.686, 0.763, 0.699, 0.763, 0.667, 0.673, 0.759, 0.708, 0.763, 0.763, 0.643, 0.724, 0.684, 0.762, 0.727, 0.639, 0.704, 0.688, 0.729, 0.761, 0.736, 0.712, 0.673, 0.723, 0.633, 0.754, 0.684, 0.71, 0.72, 0.721, 0.695, 0.671, 0.737, 0.721, 0.765, 0.76, 0.729, 0.762, 0.713, 0.616, 0.687, 0.694, 0.654, 0.745, 0.765, 0.669, 0.688, 0.686, 0.733, 0.693, 0.619, 0.652, 0.698, 0.684, 0.715, 0.709, 0.682, 0.689, 0.696, 0.621, 0.762, 0.757, 0.739, 0.66, 0.69, 0.684, 0.757, 0.693, 0.688, 0.759, 0.705, 0.709, 0.67, 0.723, 0.643, 0.706, 0.7, 0.693, 0.697, 0.676, 0.721, 0.727, 0.702, 0.752, 0.663, 0.752, 0.687, 0.708, 0.658, 
        0.726, 0.73, 0.724, 0.687, 0.74, 0.657, 0.705, 0.634, 0.681, 0.619, 0.691, 0.693, 0.701, 0.619, 0.684, 0.685, 0.764, 0.762, 0.698, 0.68, 0.746, 0.643, 0.742, 0.726, 0.743, 0.741, 0.74, 0.711, 0.756, 0.685, 0.718, 0.703, 0.657, 0.71, 0.681, 0.743, 0.726, 0.698, 0.727, 0.71, 0.717, 0.725, 0.681, 0.627, 0.679, 0.616, 0.621, 0.68, 0.662, 0.724, 0.714, 0.652, 0.698, 0.68, 0.724, 0.728, 0.675, 0.69, 0.767, 0.764, 0.682, 0.657, 0.729, 0.681, 0.661, 0.737, 0.725, 0.683, 0.705, 0.681, 0.766, 0.683, 
        0.723, 0.686, 0.691, 0.706, 0.73, 0.728, 0.613, 0.765, 0.736, 0.681, 0.688, 0.746, 0.688, 
        0.687, 0.753, 0.686, 0.706, 0.677, 0.755, 0.667, 0.687, 0.727, 0.728, 0.696, 0.701, 0.726, 0.677, 0.637, 0.75, 0.691, 0.621, 0.682, 0.687, 0.691, 0.728, 0.757, 0.729, 0.73, 0.627, 
        0.649, 0.712, 0.681, 0.69, 0.763, 0.714, 0.68, 0.759, 0.628, 0.687, 0.633, 0.683, 0.676, 0.639, 0.766, 0.749, 0.755, 0.683, 0.761, 0.729, 0.728, 0.702, 0.688, 0.616, 0.673, 0.69, 0.706, 0.727, 0.737, 0.745, 0.689, 0.623, 0.753, 0.694, 0.754, 0.71, 0.67, 0.67, 0.689, 0.731, 0.384]
    trainingDataset2Outputs2 = [0.828, 0.82, 0.824, 0.822, 0.821, 0.836, 0.86, 0.86, 0.848, 0.873, 0.882, 0.799, 0.823, 0.803, 0.873, 0.852, 0.852, 0.877, 0.804, 0.853, 0.882, 0.825, 0.877, 0.852, 0.834, 0.803, 
        0.867, 0.836, 0.85, 0.852, 0.831, 0.843, 0.796, 0.851, 0.823, 0.872, 0.859, 0.807, 0.825, 
        0.874, 0.854, 0.84, 0.855, 0.877, 0.84, 0.851, 0.876, 0.846, 0.834, 0.837, 0.826, 0.848, 0.821, 0.824, 0.85, 0.847, 0.832, 0.845, 0.799, 0.828, 0.821, 0.826, 0.802, 0.845, 0.822, 0.824, 0.848, 0.828, 0.797, 0.808, 0.804, 0.864, 0.827, 0.85, 0.802, 0.817, 0.823, 0.84, 0.814, 0.821, 0.821, 0.843, 0.821, 0.85, 0.828, 0.824, 0.824, 0.866, 0.822, 0.848, 0.845, 0.828, 0.834, 0.826, 0.805, 0.862, 0.833, 0.833, 0.851, 0.81, 0.83, 0.824, 0.828, 0.827, 0.843, 0.837, 0.838, 0.857, 0.831, 0.827, 0.83, 
        0.807, 0.882, 0.873, 0.821, 0.871, 0.828, 0.861, 0.812, 0.878, 0.825, 0.826, 0.865, 0.846, 0.842, 0.839, 0.826, 0.803, 0.83, 0.834, 0.864, 0.862, 0.816, 0.836, 0.872, 0.814, 0.809, 0.823, 0.87, 0.85, 0.832, 0.824, 0.86, 0.818, 0.851, 0.854, 0.822, 0.812, 0.831, 0.833, 0.819, 0.85, 0.82, 0.85, 0.844, 0.871, 0.799, 
        0.852, 0.849, 0.851, 0.81, 0.807, 0.855, 0.849, 0.824, 0.824, 0.861, 0.822, 0.834, 0.821, 
        0.826, 0.826, 0.816, 0.852, 0.849, 0.843, 0.823, 0.831, 0.83, 0.874, 0.857, 0.854, 0.817, 
        0.832, 0.832, 0.852, 0.854, 0.836, 0.848, 0.842, 0.825, 0.836, 0.826, 0.849, 0.835, 0.847, 0.837, 0.876, 0.824, 0.822, 0.805, 0.849, 0.852, 0.875, 0.798, 0.822, 0.873, 0.795, 0.82, 0.85, 0.849, 0.847, 0.817, 0.852, 0.844, 0.811, 0.848, 0.832, 0.824, 0.853, 0.842, 0.848, 0.865, 0.844, 0.821, 0.879, 0.823, 0.825, 0.811, 0.824, 0.824, 0.823, 0.852, 0.851, 0.852, 0.82, 0.825, 0.869, 0.866, 0.83, 0.806, 0.81, 0.878, 0.813, 0.795, 0.847, 0.826, 0.793, 
        0.82, 0.87, 0.853, 0.817, 0.871, 0.851, 0.88, 0.847, 0.881, 0.826, 0.82, 0.828, 0.815, 0.872, 0.836, 0.86, 0.839, 0.823, 0.85, 0.855, 0.861, 0.87, 0.799, 0.847, 0.833, 0.799, 0.829, 0.824, 0.851, 0.85, 0.83, 0.824, 0.849, 0.845, 0.826, 0.881, 0.824, 0.879, 0.793, 0.87, 
        0.814, 0.88, 0.834, 0.839, 0.842, 0.879, 0.87, 0.807, 0.821, 0.839, 0.84, 0.85, 0.798, 0.875, 0.863, 0.815, 0.853, 0.822, 0.848, 0.833, 0.796, 0.816, 0.846, 0.797, 0.828, 0.835, 0.806, 0.821, 0.847, 0.832, 0.85, 0.805, 0.838, 0.825, 0.819, 0.851, 0.804, 0.825, 0.838, 0.824, 0.837, 0.797, 0.812, 0.847, 0.849, 0.862, 0.819, 0.824, 0.866, 0.827, 0.845, 0.827, 0.846, 0.852, 0.797, 0.798, 0.839, 0.883, 0.85, 0.811, 0.879, 0.853, 0.82, 0.869, 0.796, 0.854, 0.851, 0.825, 0.824, 0.831, 0.825, 0.818, 0.834, 0.829, 0.861, 0.871, 0.806, 0.848, 0.796, 0.796, 0.85, 0.829, 0.838, 0.835, 0.848, 0.823, 0.841, 0.866, 0.847, 0.851, 0.85, 0.851, 0.851, 0.795, 0.826, 0.807, 0.872, 0.853, 0.797, 0.879, 0.797, 0.844, 0.824, 0.832, 0.821, 0.851, 0.848, 0.88, 0.853, 0.803, 0.842, 0.849, 0.878, 0.821, 0.818, 0.795, 0.85, 0.848, 0.847, 0.853, 0.831, 0.842, 0.794, 0.824, 0.823, 0.868, 0.826, 0.855, 0.81, 0.802, 0.827, 0.799, 0.826, 0.834, 0.85, 0.862, 0.855, 0.847, 0.874, 0.827, 0.832, 0.876, 0.822, 0.811, 0.836, 0.845, 0.878, 0.829, 0.853, 0.849, 0.876, 0.827, 0.878, 0.821, 0.822, 0.852, 0.834, 0.84, 0.848, 0.849, 0.831, 0.851, 0.848, 0.826, 0.801, 0.811, 0.823, 0.828, 0.863, 0.804, 0.802, 0.85, 0.822, 0.851, 0.824, 0.821, 0.872, 0.826, 0.86, 0.837, 0.825, 0.826, 0.865, 0.85, 0.825, 0.846, 0.827, 0.799, 0.832, 0.823, 0.82, 0.832, 0.84, 0.824, 0.873, 0.822, 0.823, 0.83, 0.801, 0.872, 0.852, 0.825, 0.804, 0.828, 0.837, 0.838, 0.875, 0.857, 0.845, 0.832, 0.836, 0.868, 0.805, 0.87, 0.875, 0.84, 0.795, 0.86, 0.795, 0.829, 0.847, 0.826, 0.822, 0.824, 0.866, 0.862, 0.846, 0.877, 0.826, 0.797, 0.817, 0.811, 0.854, 0.88, 0.805, 0.818, 0.803, 0.849, 0.831, 0.823, 0.85, 0.876, 0.848, 0.801, 0.838, 0.823, 0.839, 0.847, 0.83, 0.822, 0.833, 0.854, 0.877, 0.85, 0.848, 0.835, 0.829, 0.853, 0.836, 0.86, 0.824, 0.852, 0.846, 0.831, 0.838, 0.846, 0.831, 0.823, 0.833, 0.876, 0.84, 0.833, 0.84, 0.824, 0.876, 0.877, 0.809, 0.844, 0.794, 0.847, 0.849, 0.853, 0.827, 0.864, 0.821, 0.86, 0.821, 0.823, 0.882, 0.854, 0.875, 0.82, 0.839, 0.834, 0.823, 0.808, 0.882, 0.852, 0.856, 0.836, 0.807, 0.871, 0.825, 0.842, 0.828, 0.825, 0.812, 0.864, 0.823, 0.87, 0.849, 0.826, 0.88, 0.821, 0.88, 0.799, 0.87, 0.841, 0.823, 0.846, 0.847, 0.852, 0.816, 0.862, 0.798, 0.883, 0.842, 0.832, 0.849, 0.841, 0.824, 0.862, 0.822, 
        0.83, 0.882, 0.824, 0.844, 0.832, 0.823, 0.859, 0.823, 0.866, 0.839, 0.834, 0.811, 0.823, 
        0.871, 0.829, 0.851, 0.846, 0.854, 0.833, 0.848, 0.862, 0.851, 0.85, 0.876, 0.85, 0.822, 0.824, 0.844, 0.851, 0.882, 0.822, 0.826, 0.866, 0.85, 0.847, 0.859, 0.854, 0.873, 0.851, 0.879, 0.848, 0.801, 0.852, 0.802, 0.875, 0.831, 0.851, 0.84, 0.838, 0.837, 0.829, 0.826, 0.84, 0.881, 0.83, 0.837, 0.827, 0.853, 0.814, 0.803, 0.832, 0.849, 0.822, 0.849, 0.852, 0.854, 0.858, 0.852, 0.828, 0.844, 0.83, 0.828, 0.797, 0.809, 0.84, 0.829, 0.822, 0.878, 0.821, 0.827, 0.825, 0.841, 0.836, 0.852, 0.85, 
        0.877, 0.848, 0.812, 0.812, 0.843, 0.811, 0.852, 0.855, 0.824, 0.823, 0.823, 0.848, 0.859, 0.851, 0.825, 0.824, 0.845, 0.848, 0.851, 0.825, 0.882, 0.875, 0.853, 0.855, 0.847, 0.827, 0.869, 0.829, 0.823, 0.833, 0.835, 0.877, 0.841, 0.871, 0.827, 0.822, 0.849, 0.852, 0.821, 0.824, 0.795, 0.843, 0.878, 0.875, 0.801, 0.813, 0.831, 0.852, 0.836, 0.846, 0.826, 0.841, 0.802, 0.846, 0.8, 0.835, 0.799, 0.854, 0.85, 0.803, 0.84, 0.798, 0.799, 0.866, 0.831, 0.853, 0.8, 0.823, 0.869, 0.833, 0.85, 0.824, 0.801, 0.823, 0.828, 0.851, 0.824, 0.871, 0.808, 0.851, 0.84, 0.825, 0.833, 0.837, 0.851, 0.822, 0.829, 0.797, 0.802, 0.821, 0.799, 0.829, 0.881, 0.849, 0.847, 0.86, 0.816, 0.796, 0.852, 0.842, 0.85, 0.825, 0.848, 0.88, 0.861, 0.846, 0.848, 0.837, 0.829, 0.85, 0.851, 0.838, 0.879, 0.798, 0.806, 0.821, 0.857, 0.84, 0.848, 0.805, 0.848, 0.842, 0.802, 0.842, 0.83, 0.852, 0.823, 0.866, 0.842, 0.845, 0.847, 0.837, 0.848, 0.824, 0.823, 0.844, 0.809, 0.855, 0.81, 0.846, 0.84, 0.859, 0.822, 0.827, 0.831, 0.846, 0.82, 0.858, 0.842, 0.872, 0.852, 0.879, 0.848, 0.848, 0.845, 0.879, 0.85, 0.848, 0.798, 0.8, 0.836, 0.852, 0.815, 0.867, 0.818, 0.83, 0.818, 0.819, 0.819, 0.839, 0.806, 
        0.851, 0.828, 0.833, 0.859, 0.829, 0.85, 0.817, 0.823, 0.836, 0.823, 0.83, 0.825, 0.826, 0.845, 0.875, 0.852, 0.881, 0.878, 0.851, 0.856, 0.831, 0.827, 0.861, 0.836, 0.853, 0.828, 0.823, 0.849, 0.84, 0.795, 0.796, 0.852, 0.859, 0.822, 0.846, 0.857, 0.822, 0.825, 0.848, 0.842, 0.852, 0.796, 0.853, 0.824, 0.85, 0.841, 0.833, 0.828, 0.823, 0.882, 0.795, 0.823, 0.851, 0.85, 0.815, 0.841, 0.846, 0.809, 0.843, 0.831, 0.853, 0.807, 0.853, 0.85, 0.821, 0.828, 0.847, 0.834, 0.823, 0.852, 0.869, 0.811, 0.84, 0.877, 0.852, 0.843, 0.84, 0.824, 0.805, 0.824, 0.822, 0.875, 0.864, 0.839, 0.849, 0.84, 0.797, 0.828, 0.851, 0.803, 0.875, 0.842, 0.871, 0.85, 0.854, 0.869, 0.795, 0.812, 0.807, 0.851, 0.801, 0.828, 0.824, 0.844, 0.849, 0.881, 0.849, 0.849, 0.841, 0.822, 0.822, 0.816, 0.842, 0.878, 0.808, 0.848, 0.808, 0.83, 0.852, 0.852, 0.849, 0.821, 0.497]
    #testingDataset2
    testingDataset2Outputs1 = [0.726, 0.718, 0.691, 0.699, 0.742, 0.643, 0.741, 0.67, 0.676, 0.765, 0.639, 0.636, 0.636, 0.738, 0.711, 0.695, 0.648, 0.73, 0.701, 0.654, 0.77, 0.662, 0.659, 0.721, 0.764, 0.644, 0.71, 0.698, 0.741, 0.766, 0.774, 0.64, 0.732, 0.769, 0.716, 0.641, 0.707, 0.77, 
        0.708, 0.7, 0.693, 0.735, 0.715, 0.705, 0.655, 0.727, 0.741, 0.685, 0.759, 0.635, 0.736, 0.727, 0.732, 0.635, 0.671, 0.694, 0.642, 0.764, 0.753, 0.769, 0.738, 0.736, 0.699, 0.69, 0.693, 0.682, 0.77, 0.72, 0.755, 0.711, 0.696, 0.678, 0.767, 0.677, 0.728, 0.634, 
        0.679, 0.722, 0.749, 0.689, 0.71, 0.649, 0.717, 0.709, 0.716, 0.695, 0.633, 0.73, 0.707, 0.726, 0.748, 0.693, 0.733, 0.725, 0.69, 0.702, 0.661, 0.757, 0.762, 0.75, 0.631, 0.728, 0.707, 0.744, 0.705, 0.694, 0.642, 0.667, 0.705, 0.767, 0.697, 0.692, 0.674, 0.733, 0.734, 0.661, 0.632, 0.662, 0.704, 0.77, 0.7, 0.693, 0.755, 0.691, 0.771, 0.662, 0.771, 0.713, 0.732, 0.734, 0.74, 0.688, 0.698, 0.695, 0.65, 0.742, 0.713, 0.737, 0.725, 0.724, 0.741, 0.696, 0.709, 0.735, 0.635, 0.758, 0.698, 0.696, 0.702, 0.712, 0.744, 0.765, 0.748, 0.759, 0.632, 0.719, 0.763, 0.717, 0.682, 0.772, 0.737, 0.771, 0.736, 0.772, 0.727, 0.776, 0.741, 0.714, 0.636, 0.652, 0.631, 0.719, 0.766, 0.691, 0.747, 0.736, 0.754, 0.636, 0.666, 0.766, 0.635, 0.684, 0.737, 0.652, 0.713, 0.759, 0.736, 0.633, 0.737, 0.772, 0.747, 0.729, 0.737, 0.768, 0.701, 0.773, 0.753, 0.726, 0.734, 0.36]
    testingDataset2Outputs2 = [0.818, 0.834, 0.836, 0.843, 0.812, 0.864, 0.82, 0.848, 0.844, 0.798, 0.866, 0.866, 0.869, 0.816, 0.838, 0.84, 0.862, 0.816, 0.841, 0.858, 0.792, 0.852, 0.854, 0.821, 0.801, 
        0.864, 0.837, 0.832, 0.82, 0.798, 0.788, 0.865, 0.816, 0.794, 0.823, 0.865, 0.828, 0.79, 0.827, 0.842, 0.834, 0.815, 0.835, 0.839, 0.856, 0.829, 0.816, 0.838, 0.804, 0.868, 0.816, 0.83, 0.816, 0.87, 0.847, 0.843, 0.864, 0.799, 0.812, 0.794, 0.816, 0.815, 0.832, 0.844, 0.834, 0.841, 0.792, 0.834, 0.809, 0.838, 0.843, 0.843, 0.796, 0.843, 0.829, 0.871, 0.842, 0.819, 0.814, 0.843, 0.838, 0.86, 0.823, 0.838, 0.835, 0.839, 0.87, 0.828, 
        0.827, 0.829, 0.816, 0.835, 0.825, 0.819, 0.843, 0.84, 0.853, 0.808, 0.803, 0.814, 0.872, 0.828, 0.839, 0.819, 0.828, 0.833, 0.865, 0.849, 0.829, 0.794, 0.84, 0.842, 0.844, 0.815, 0.813, 0.852, 0.871, 0.853, 0.829, 0.792, 0.841, 0.844, 0.808, 0.841, 0.792, 0.853, 0.791, 0.824, 0.827, 0.815, 0.821, 0.842, 0.842, 0.843, 0.86, 0.819, 0.836, 0.817, 0.831, 0.818, 0.816, 0.843, 0.826, 0.815, 0.872, 0.806, 0.842, 0.833, 0.841, 0.824, 0.819, 0.8, 0.815, 0.806, 0.872, 0.821, 0.801, 0.834, 0.841, 0.789, 0.813, 0.788, 0.818, 0.79, 0.818, 0.785, 0.814, 0.824, 0.869, 0.858, 0.873, 0.834, 0.796, 0.84, 0.815, 0.823, 
        0.809, 0.869, 0.85, 0.799, 0.87, 0.839, 0.814, 0.858, 0.838, 0.804, 0.825, 0.872, 0.813, 0.789, 0.817, 0.817, 0.816, 0.796, 0.84, 0.787, 0.811, 0.818, 0.818, 0.471]
    
    size1 =len(trainingDataset1Outputs1)
    size2 =len(trainingDataset1Outputs2)
    size3 = len(testingDataset1Outputs1)
    size4 = len(testingDataset1Outputs2)
    size5 = len(trainingDataset2Outputs1)
    size6 = len(trainingDataset2Outputs2)
    size7 = len(testingDataset2Outputs1)
    size8 = len(testingDataset2Outputs2)
    # 500 500 180 180 1000 1000 200 200 size of each
    #pick minimum = 180
    AccuracyForTrainingDataset1X = [0 for i in range(180)]
    AccuracyForTrainingDataset1Y = [0 for i in range(180)]

    AccuracyForTrainingDataset2X = [0 for i in range(180)]
    AccuracyForTrainingDataset2Y = [0 for i in range(180)]
    #((training - testing)/ testing)*100 = AccuracyError
    #training dataset 1 and training dataset 2's accuracy is averaged averaged.
    for i in range(180):
        AccuracyOfoutput1D1X = ((((trainingDataset1Outputs1[i])*(testingDataset1Outputs1[i]))/testingDataset1Outputs1[i])*100)
        AccuracyOfoutput2D1Y = ((((trainingDataset1Outputs2[i])*(testingDataset1Outputs2[i]))/testingDataset1Outputs2[i])*100)
        AccuracyForTrainingDataset1X[i] = AccuracyOfoutput1D1X;
        AccuracyForTrainingDataset1Y[i] = AccuracyOfoutput2D1Y;

        AccuracyOfoutput1D2 = ((((trainingDataset2Outputs1[i])*(testingDataset2Outputs1[i]))/testingDataset2Outputs1[i])*100)
        AccuracyOfoutput2D2 = ((((trainingDataset2Outputs2[i])*(testingDataset2Outputs2[i]))/testingDataset2Outputs2[i])*100)
        AccuracyForTrainingDataset2X[i] = AccuracyOfoutput1D2
        AccuracyForTrainingDataset2Y[i] = AccuracyOfoutput2D2;
    
    # fistst 180
    clusterX = [0 for i in range(81)] # average distance < 50
    clusterY = [0 for i in range(81)] # > 50

    clusterX2 = [0 for i in range(81)]
    clusterY2 = [0 for i in range(81)]




    it = 0;
    for i in range(len(AccuracyForTrainingDataset1X)):
        AverageDisBW = (AccuracyForTrainingDataset1X[i] - AccuracyForTrainingDataset2X[i])**2 + (AccuracyForTrainingDataset1Y[i] - AccuracyForTrainingDataset2Y[i])**2
        if(AverageDisBW > 50):
            clusterX[it] = trainingDatatset1[i][2]
            clusterY[it] = trainingDatatset1[i][3]
            
            clusterX2[it] = trainingDatatset2[i][0]
            clusterY2[it] = trainingDatatset2[i][1]
            it+=1
            #print("%d|\t%f|\t%f|\t%f|\t%f|\t%f"
                # %(i+1, AccuracyForTrainingDataset1X[i],
                # AccuracyForTrainingDataset1Y[i],
                # AccuracyForTrainingDataset2X[i],
                # AccuracyForTrainingDataset2Y[i], AverageDisBW)
                # )
    
    print(clusterX2)
    print();
    print(clusterY2) 

    x = clusterX;
    y = clusterY;
    x2 = clusterX2;
    y2 = clusterY2;
    
    

    #DecisoinBoundary
    plt.scatter(x,y)
    plt.scatter(x2,y2)
    plt.plot(x2, y2)
    plt.show()
# results();
AccuracyCalCulation()

